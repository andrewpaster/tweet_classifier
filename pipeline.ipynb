{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for Vectorizing and Training on Twitter Disaster Message Data\n",
    "* twitter messages are vectorized using the spacy (tokenization )and gensim (tfidf) libraries\n",
    "* steps include:\n",
    "  * all strings to lowercase\n",
    "  * tokenization\n",
    "  * stopword removal\n",
    "  * lemmatization\n",
    "  * bag of words\n",
    "  * tfidf\n",
    "  * GloVe vectors\n",
    "  * Count of the number of hashtags\n",
    "  \n",
    " A custom scikit learn transformer is created for scikit learn pipelining. That way cross-validation can be done properly where the text corpus is made from training data rather than from training and cross-validation data.\n",
    " \n",
    " Multiple models were tried including:\n",
    " * Gaussian Naive Bayes\n",
    " * SVM with singular value decomposition to decrease the number of features\n",
    " * Decision Tree\n",
    " * Gradient Boosted Trees\n",
    " * Logistic Regression\n",
    " \n",
    "Logistic Regression and Gradient Boosted Trees had similar performance with a Relevant recall of .73. Because Logistic Regression is simpler and faster to train, that was used for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy\n",
    "import gensim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('socialmedia-disaster-tweets-DFE.csv', encoding='latin-1')\n",
    "df = df[(df['choose_one'] == 'Relevant') | (df['choose_one'] == 'Not Relevant')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10860, 13)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.text\n",
    "y = df.choose_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   Just happened a terrible car crash\n",
       "1    Our Deeds are the Reason of this #earthquake M...\n",
       "2    Heard about #earthquake is different cities, s...\n",
       "3    there is a forest fire at spot pond, geese are...\n",
       "4               Forest fire near La Ronge Sask. Canada\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex for replacing links and twitter handles\n",
    "regex_link = re.compile(\"(?P<url>https?://[^\\s]+)\")\n",
    "regex_handle = re.compile('(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)')\n",
    "\n",
    "def regex_replace(text):\n",
    "    \"\"\"replace weblinks with the word webpage and twitter handles with the word username\n",
    "    \n",
    "    Args:\n",
    "       text (str): a string with a Twitter message\n",
    "        \n",
    "    Returns:\n",
    "        prepared_text (str): a string with weblinks and twitter handles replaced\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    prepared_text = regex_link.sub('webpage', text)\n",
    "    prepared_text = regex_handle.sub('username', prepared_text)\n",
    "\n",
    "    return prepared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare text as vectors\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# fixes the issue with the spacy library where stop words are not included with the model\n",
    "for word in nlp.Defaults.stop_words:\n",
    "    nlp.vocab[word].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make word lowercase\n",
    "def lowercase(text):\n",
    "    \"\"\"make string lowercase\n",
    "    \n",
    "    Args:\n",
    "        text (str): a string\n",
    "    \n",
    "    Returns:\n",
    "        text (str): a string with all lowercase letters\n",
    "    \"\"\"\n",
    "    \n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output tokens for tweet, GloVe vectors, and count of hashtags\n",
    "def spacy_tokenize_glove(text):\n",
    "    \"\"\"prepares a Tweet for further processing\n",
    "    \n",
    "    Args:\n",
    "        text (str): a string representing a Tweet\n",
    "        \n",
    "    Returns:\n",
    "        tokenized_text (list(str)): a list of tokenized words\n",
    "        doc_vector (list(float)): a vector of average GloVe for tokenizes in the text\n",
    "        hashtag_counts (int): number of hashtags used in the Tweet\n",
    "    \n",
    "    \"\"\"\n",
    "    text = regex_replace(text)\n",
    "    text = lowercase(text)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    doc_vector = [] # holds the GloVe vector for each token as a list\n",
    "    hashtag_counts = 0 # number of hashtags in the tweet\n",
    "    tokenized_text = [] # tokenized text\n",
    "    \n",
    "    for token in doc:\n",
    "        # only keep words that are not punctuation, space, or stop words\n",
    "        if token.is_stop != True and token.is_punct != True and token.text.isspace() != True: \n",
    "            if token.lemma_ != '-PRON-': \n",
    "                tokenized_text.append(token.lemma_)\n",
    "            else:\n",
    "                tokenized_text.append(token) # keep pronouns in original form though most if not all are stop words\n",
    "                \n",
    "            doc_vector.append(token.vector) # append the vector fo the token, which will be averaged\n",
    "        \n",
    "        if token.text == '#':\n",
    "            hashtag_counts += 1\n",
    "            \n",
    "    if len(doc_vector) > 0:\n",
    "        doc_vector = np.mean(doc_vector, axis=0)\n",
    "    else:\n",
    "        doc_vector = np.array([0]*300) # for a tweet with no word embedding vector\n",
    "\n",
    "    return tokenized_text, doc_vector, hashtag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a corpus of tokenized words\n",
    "def create_corpus(data):\n",
    "    \"\"\"creates a corpus of tokenized tweets, GloVe doc vectors, and counts of hashtags\n",
    "    \n",
    "    Args: \n",
    "        data (list(str)): list of unprocessed tweets\n",
    "    \n",
    "    Returns:\n",
    "        corpus (list(list)): list of tokenized tweets\n",
    "        vector_corpus (list(list)): list of GloVe vector for each tweet\n",
    "        hashtag_corpus (list(int)): list of hashtag counts for each tweet\n",
    "        frequency (dict): dictionary of frequency counts for words in corpus\n",
    "    \"\"\"\n",
    "    frequency = Counter()\n",
    "    corpus = []\n",
    "    vector_corpus = []\n",
    "    hashtag_corpus = []\n",
    "    \n",
    "    for tweet in data:\n",
    "                \n",
    "        tokenized_text, doc_vector, hashtag_counts = spacy_tokenize_glove(tweet)\n",
    "        \n",
    "        corpus.append(tokenized_text)\n",
    "        vector_corpus.append(doc_vector)\n",
    "        hashtag_corpus.append(hashtag_counts)\n",
    "        \n",
    "        for token in tokenized_text:\n",
    "            frequency[token] += 1\n",
    "    \n",
    "    # removes tokens that only appear once in the corpus\n",
    "    corpus = [[token for token in tweet if frequency[token] > 1] for tweet in corpus]\n",
    "    \n",
    "    return corpus, vector_corpus, hashtag_corpus, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_tfidf_model(corpus):\n",
    "    \"\"\"outputs a tfidf model from a corpus\n",
    "    \n",
    "    Args:\n",
    "        corpus (list(list)): list of tweets as tokens\n",
    "        \n",
    "    Returns:\n",
    "        tfidf: tfidf model from gensim\n",
    "        \n",
    "    \"\"\"\n",
    "    dictionary = gensim.corpora.Dictionary(corpus)\n",
    "    bow = [dictionary.doc2bow(text) for text in corpus]\n",
    "    tfidf = gensim.models.TfidfModel(bow, normalize=True)\n",
    "    \n",
    "    return tfidf, bow, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_new_text(text, corpus_dictionary, tfidf_model, frequency):\n",
    "    \"\"\"take a list of text messages and output features\n",
    "    \n",
    "        Args:\n",
    "            text (list): list of text messages\n",
    "            corpus_dictionary (dict): mapping of words to ids\n",
    "            tfidf_model (tfidf): gensim tfidf model\n",
    "            frequency (dict): word count of words in corpus\n",
    "            \n",
    "        Returns:\n",
    "            tfidf_features (list): list of tfidf features for each message\n",
    "            doc_vector (list): list of GloVe vectors for each message\n",
    "            hashtag_counts (list): list of hashtag counts for each message\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized_text, doc_vector, hashtag_counts = spacy_tokenize_glove(text)\n",
    "\n",
    "    # removes tokens that only appear once in the corpus\n",
    "    tokenized_text = [token for token in tokenized_text if frequency[token] > 1]\n",
    "\n",
    "    bow = dictionary.doc2bow(tokenized_text)\n",
    "\n",
    "    tfidf_features = tfidf_model[bow]\n",
    "    \n",
    "    return tfidf_features, doc_vector, hashtag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(tfidf_features, doc_vector, hashtag_counts, terms):\n",
    "    \"\"\"concatenate different features into a single vector and scale the hashtag counts\n",
    "    \n",
    "    Args:\n",
    "        tfidf_features (list): list of tfidf features for each message\n",
    "        doc_Vector (list): list of GloVe features for each message\n",
    "        hashtag_counts (list): list of hashtag counts for each message\n",
    "        terms (int): size of the corpus vocabulary\n",
    "    \n",
    "    Returns:\n",
    "        features (list): concatenated features\n",
    "        scaler (MinMaxScaler): scaler for scaling hashtags on prediction data\n",
    "    \"\"\"\n",
    "    \n",
    "    np_tfidf = gensim.matutils.corpus2dense(tfidf_features, num_terms = terms).T\n",
    "    scaler = MinMaxScaler()\n",
    "    hashtag_count = scaler.fit_transform([[num] for num in hashtag_counts])\n",
    "    \n",
    "    features = np.hstack((np_tfidf, hashtag_count, doc_vector))\n",
    "    \n",
    "    return features, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_predict_features(tfidf_features, doc_vector, hashtag_counts, terms, scaler):\n",
    "    \"\"\"concatenate different features into a single vector and scale hashtag counts for single row\n",
    "    of prediction data\n",
    "    \n",
    "    Args:\n",
    "        tfidf_features (list): tfidf features message\n",
    "        doc_Vector (list): GloVe features for message\n",
    "        hashtag_counts (list): lhashtag counts message\n",
    "        terms (int): size of the corpus vocabulary\n",
    "        scaler (object): MinMaxScaler from training\n",
    "    \n",
    "    Returns:\n",
    "        features (list): concatenated features\n",
    "        scaler (MinMaxScaler): scaler for scaling hashtags on prediction data\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    np_tfidf = gensim.matutils.corpus2dense([tfidf_features], num_terms = terms).T\n",
    "    hashtag_count = scaler.fit_transform([[hashtag_counts]])\n",
    "    features = np.hstack((np_tfidf, hashtag_count, [doc_vector]))\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_training_features(train_data):\n",
    "    \"\"\"runs messages through the entire pipeline to output training features \n",
    "    \n",
    "    Args:\n",
    "        train_data (list): list of messages \n",
    "        \n",
    "    Returns:\n",
    "        X_train (list): training features for each message\n",
    "        tfidf_model (object): tfidf model to be used to vectorize prediction data\n",
    "        scaler (object): MinMaxScaler for hashtags\n",
    "        dictionary (dict): mappings of ids to words\n",
    "        frequency (dict): frequency counts of words in corpus\n",
    "    \n",
    "    \"\"\"\n",
    "    corpus, vector_corpus, hashtag_corpus, frequency = create_corpus(X[0:200]) # create a corpus and other features\n",
    "    tfidf_model, bow, dictionary = output_tfidf_model(corpus) # tfidf model and bag of words from corpus\n",
    "    X_train, scaler = prepare_train_features(tfidf_model[bow], vector_corpus, hashtag_corpus, len(dictionary))\n",
    "    \n",
    "    return X_train, tfidf_model, scaler, dictionary, frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_test_features(text, dictionary, tfidf_model, frequency):\n",
    "    \"\"\"runs new messages through the pipeline using the training corpus\n",
    "    \n",
    "    Args:\n",
    "        text (str): new text to predict on\n",
    "        dictionary (dict): mapping of ids to words\n",
    "        tfidf_model (object): model from the training corpus\n",
    "        frequency (dict): frequency counts for words in the model\n",
    "    \n",
    "    Returns:\n",
    "        features (list): features for the new text\n",
    "    \"\"\"\n",
    "    \n",
    "    tfidf_features, doc_vector, hashtag_counts = pipeline_new_text(text, dictionary, tfidf_model, frequency)\n",
    "    features = prepare_predict_features(tfidf_features, doc_vector, hashtag_counts, len(dictionary), scaler)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.74977612e-01  6.87750220e-01  5.48995376e-01  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00 -8.03157613e-02  3.44924986e-01\n",
      "  -1.08413495e-01  1.25625253e-01  5.15295118e-02 -1.40821502e-01\n",
      "   6.10400029e-02 -1.34355009e-01 -4.12345082e-02  2.48274994e+00\n",
      "  -1.94113255e-01 -5.94882518e-02 -7.90400151e-03  3.75173986e-01\n",
      "  -7.33102500e-01 -2.22572505e-01 -2.15984806e-01  7.50446260e-01\n",
      "   3.12465847e-01 -1.92011744e-01 -3.41337509e-02 -1.44877508e-01\n",
      "   2.23065764e-01 -2.94920001e-02  1.56333998e-01 -1.54044963e-02\n",
      "  -1.86232507e-01 -6.65000081e-02  8.58517587e-02 -6.25512227e-02\n",
      "   7.67700374e-03 -2.09809989e-01 -1.33417010e-01  3.19593012e-01\n",
      "  -3.89304981e-02  1.34120002e-01  1.09562486e-01  2.50475764e-01\n",
      "  -2.60132492e-01  1.75490499e-01  3.88598502e-01 -2.13848755e-01\n",
      "   2.99612544e-02  3.14980000e-01 -6.01875037e-02  9.97329950e-02\n",
      "  -2.40769252e-01 -1.44002497e-01  1.47061750e-01 -3.56040522e-02\n",
      "   8.16007555e-02 -7.48175085e-02  1.23019502e-01  3.70045006e-01\n",
      "  -3.39181244e-01  1.21299759e-01 -8.68950039e-02 -2.83802480e-01\n",
      "   2.49179989e-01 -7.68562481e-02 -1.21249996e-01 -1.00066498e-01\n",
      "   9.06025246e-03  1.93408757e-01  8.18255022e-02  1.51966244e-01\n",
      "   1.68211251e-01 -1.50424242e-01  2.23086506e-01  4.00899976e-01\n",
      "   1.56812504e-01 -4.94612455e-02  2.41968006e-01  2.49468505e-01\n",
      "   5.79625135e-03 -1.24795869e-01 -9.42707434e-03 -1.85934752e-01\n",
      "  -3.20022494e-01  3.41159999e-01  7.14595020e-02  2.22012505e-01\n",
      "   1.13612503e-01 -1.54105052e-02 -2.87014991e-02 -2.25669984e-02\n",
      "   3.45195323e-01 -3.80874991e-01 -2.02412501e-01 -8.15225095e-02\n",
      "  -2.11640745e-01  4.89389971e-02 -2.89315015e-01 -2.82595009e-01\n",
      "  -1.52325034e-02 -1.25814006e-01  4.35370021e-02  1.63642257e-01\n",
      "   4.52177525e-01 -2.89397482e-02 -1.11854494e-01  2.28671506e-01\n",
      "   1.65272489e-01 -4.85832468e-02  2.05110237e-01 -6.59262478e-01\n",
      "  -1.55414984e-01  3.31282496e-01  5.28879985e-02 -2.90904958e-02\n",
      "  -2.78805017e-01 -2.24545002e-01  1.60392880e-01 -1.60900444e-01\n",
      "  -1.51841998e-01 -8.93187523e-02  2.03407511e-01  3.56142521e-02\n",
      "  -2.07777008e-01 -1.94945484e-01 -1.69257745e-01 -2.14720249e-01\n",
      "   1.82366490e-01 -3.86902273e-01 -1.17332496e-01 -6.34552464e-02\n",
      "  -1.37942493e-01 -1.08112492e-01  7.68567547e-02 -2.06200015e-02\n",
      "   1.78835005e-01  1.08882450e-02  2.52737403e-02  5.33724949e-02\n",
      "   6.36305064e-02 -4.10745502e-01 -5.08567452e-01  1.46996245e-01\n",
      "  -1.64948076e-01  1.07575998e-01 -1.92932487e+00 -4.96705025e-02\n",
      "   5.34507513e-01  1.90057486e-01  2.61875987e-01  1.59468248e-01\n",
      "  -4.56477523e-01  1.60376757e-01  3.80856276e-01 -1.67205006e-01\n",
      "  -3.15167010e-01  1.69396251e-01  6.37339950e-02  3.50087471e-02\n",
      "   1.86181247e-01 -2.48815000e-01  1.24927752e-01  1.83575004e-01\n",
      "   2.31370002e-01 -2.25619853e-01 -1.20545253e-01  3.16525027e-02\n",
      "   3.48692477e-01  1.05028249e-01  9.11007524e-02 -2.06900053e-02\n",
      "  -2.26608992e-01  3.12472522e-01  1.69867754e-01 -5.12999669e-03\n",
      "  -1.12002760e-01  1.38169259e-01 -2.71900017e-02  1.13612249e-01\n",
      "  -1.11048751e-01 -1.32636249e-01 -7.23820031e-02  2.17517510e-01\n",
      "   6.55268028e-04 -1.92855000e-01 -9.54500213e-03 -1.85799345e-01\n",
      "  -2.21314520e-01  3.29086483e-01 -6.68975040e-02 -2.91843247e-02\n",
      "   2.57439967e-02  5.55294976e-02 -3.56500074e-02  2.57788986e-01\n",
      "   3.95379990e-01 -2.01334991e-02  7.39842504e-02  7.51900077e-02\n",
      "  -1.36788756e-01 -1.53245002e-01  8.33352506e-02 -1.53027251e-01\n",
      "  -1.24030501e-01  1.78916246e-01 -1.79174989e-01  9.36699957e-02\n",
      "   1.75162494e-01 -1.40802875e-01  2.94327457e-03  2.74553485e-02\n",
      "  -1.22877255e-01  1.41344756e-01  2.25312248e-01  3.25670004e-01\n",
      "   1.84649993e-02 -9.65782478e-02  1.72021747e-01  3.24707508e-01\n",
      "   5.58669940e-02 -2.07220003e-01 -8.94087479e-02  2.13347495e-01\n",
      "  -1.03633493e-01 -2.86620021e-01  4.15201008e-01 -9.16349962e-02\n",
      "  -1.98102996e-01  1.21238299e-01 -1.37545750e-01 -1.50137752e-01\n",
      "  -6.95754727e-03  1.86879501e-01 -5.73685020e-02 -2.71399990e-02\n",
      "  -1.07082896e-01  9.18634981e-02  7.46712461e-02  4.46032509e-02\n",
      "  -2.74399757e-01  1.09037496e-01  1.21062741e-01  3.35427523e-01\n",
      "  -1.84619501e-01  2.86572501e-02  1.09135240e-01  6.30113930e-02\n",
      "   1.31270498e-01  7.63574988e-02 -1.82839245e-01 -1.30219996e-01\n",
      "   5.23344994e-01 -1.29047632e-01  1.72891244e-01  3.35097462e-02\n",
      "  -1.94304749e-01 -1.07482508e-01  2.12738752e-01 -4.07324992e-02\n",
      "  -2.42945492e-01 -1.64375007e-02 -1.34829000e-01 -4.67239991e-02\n",
      "   1.97099984e-01  1.10903755e-01  1.48600250e-01  7.04132468e-02\n",
      "   1.43355995e-01 -3.35429996e-01  1.89768016e-01 -3.83202970e-01\n",
      "  -1.40029743e-01  5.03121972e-01 -7.79475048e-02  7.83237517e-02\n",
      "   1.84897497e-01  1.54842548e-02  3.38107497e-01 -2.12439999e-01\n",
      "  -2.66589999e-01  1.76412970e-01  1.46627501e-01 -7.39411265e-02\n",
      "   4.77957502e-02  1.14010498e-01 -2.49372259e-01 -2.75062501e-01\n",
      "  -1.73342526e-02 -4.51627560e-02  1.83018997e-01  1.39599115e-01\n",
      "   7.63474926e-02 -1.70930997e-01 -1.16726495e-01 -2.30366498e-01\n",
      "  -6.29459977e-01  1.98060006e-01  3.33459973e-01 -1.91512004e-01\n",
      "   1.81127518e-01 -1.28297508e-01 -4.48192507e-02  7.02579916e-02\n",
      "   1.99540004e-01  5.37825376e-03]]\n",
      "[ 4.74977612e-01  6.87750220e-01  5.48995376e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -8.03157613e-02  3.44924986e-01\n",
      " -1.08413495e-01  1.25625253e-01  5.15295118e-02 -1.40821502e-01\n",
      "  6.10400029e-02 -1.34355009e-01 -4.12345082e-02  2.48274994e+00\n",
      " -1.94113255e-01 -5.94882518e-02 -7.90400151e-03  3.75173986e-01\n",
      " -7.33102500e-01 -2.22572505e-01 -2.15984806e-01  7.50446260e-01\n",
      "  3.12465847e-01 -1.92011744e-01 -3.41337509e-02 -1.44877508e-01\n",
      "  2.23065764e-01 -2.94920001e-02  1.56333998e-01 -1.54044963e-02\n",
      " -1.86232507e-01 -6.65000081e-02  8.58517587e-02 -6.25512227e-02\n",
      "  7.67700374e-03 -2.09809989e-01 -1.33417010e-01  3.19593012e-01\n",
      " -3.89304981e-02  1.34120002e-01  1.09562486e-01  2.50475764e-01\n",
      " -2.60132492e-01  1.75490499e-01  3.88598502e-01 -2.13848755e-01\n",
      "  2.99612544e-02  3.14980000e-01 -6.01875037e-02  9.97329950e-02\n",
      " -2.40769252e-01 -1.44002497e-01  1.47061750e-01 -3.56040522e-02\n",
      "  8.16007555e-02 -7.48175085e-02  1.23019502e-01  3.70045006e-01\n",
      " -3.39181244e-01  1.21299759e-01 -8.68950039e-02 -2.83802480e-01\n",
      "  2.49179989e-01 -7.68562481e-02 -1.21249996e-01 -1.00066498e-01\n",
      "  9.06025246e-03  1.93408757e-01  8.18255022e-02  1.51966244e-01\n",
      "  1.68211251e-01 -1.50424242e-01  2.23086506e-01  4.00899976e-01\n",
      "  1.56812504e-01 -4.94612455e-02  2.41968006e-01  2.49468505e-01\n",
      "  5.79625135e-03 -1.24795869e-01 -9.42707434e-03 -1.85934752e-01\n",
      " -3.20022494e-01  3.41159999e-01  7.14595020e-02  2.22012505e-01\n",
      "  1.13612503e-01 -1.54105052e-02 -2.87014991e-02 -2.25669984e-02\n",
      "  3.45195323e-01 -3.80874991e-01 -2.02412501e-01 -8.15225095e-02\n",
      " -2.11640745e-01  4.89389971e-02 -2.89315015e-01 -2.82595009e-01\n",
      " -1.52325034e-02 -1.25814006e-01  4.35370021e-02  1.63642257e-01\n",
      "  4.52177525e-01 -2.89397482e-02 -1.11854494e-01  2.28671506e-01\n",
      "  1.65272489e-01 -4.85832468e-02  2.05110237e-01 -6.59262478e-01\n",
      " -1.55414984e-01  3.31282496e-01  5.28879985e-02 -2.90904958e-02\n",
      " -2.78805017e-01 -2.24545002e-01  1.60392880e-01 -1.60900444e-01\n",
      " -1.51841998e-01 -8.93187523e-02  2.03407511e-01  3.56142521e-02\n",
      " -2.07777008e-01 -1.94945484e-01 -1.69257745e-01 -2.14720249e-01\n",
      "  1.82366490e-01 -3.86902273e-01 -1.17332496e-01 -6.34552464e-02\n",
      " -1.37942493e-01 -1.08112492e-01  7.68567547e-02 -2.06200015e-02\n",
      "  1.78835005e-01  1.08882450e-02  2.52737403e-02  5.33724949e-02\n",
      "  6.36305064e-02 -4.10745502e-01 -5.08567452e-01  1.46996245e-01\n",
      " -1.64948076e-01  1.07575998e-01 -1.92932487e+00 -4.96705025e-02\n",
      "  5.34507513e-01  1.90057486e-01  2.61875987e-01  1.59468248e-01\n",
      " -4.56477523e-01  1.60376757e-01  3.80856276e-01 -1.67205006e-01\n",
      " -3.15167010e-01  1.69396251e-01  6.37339950e-02  3.50087471e-02\n",
      "  1.86181247e-01 -2.48815000e-01  1.24927752e-01  1.83575004e-01\n",
      "  2.31370002e-01 -2.25619853e-01 -1.20545253e-01  3.16525027e-02\n",
      "  3.48692477e-01  1.05028249e-01  9.11007524e-02 -2.06900053e-02\n",
      " -2.26608992e-01  3.12472522e-01  1.69867754e-01 -5.12999669e-03\n",
      " -1.12002760e-01  1.38169259e-01 -2.71900017e-02  1.13612249e-01\n",
      " -1.11048751e-01 -1.32636249e-01 -7.23820031e-02  2.17517510e-01\n",
      "  6.55268028e-04 -1.92855000e-01 -9.54500213e-03 -1.85799345e-01\n",
      " -2.21314520e-01  3.29086483e-01 -6.68975040e-02 -2.91843247e-02\n",
      "  2.57439967e-02  5.55294976e-02 -3.56500074e-02  2.57788986e-01\n",
      "  3.95379990e-01 -2.01334991e-02  7.39842504e-02  7.51900077e-02\n",
      " -1.36788756e-01 -1.53245002e-01  8.33352506e-02 -1.53027251e-01\n",
      " -1.24030501e-01  1.78916246e-01 -1.79174989e-01  9.36699957e-02\n",
      "  1.75162494e-01 -1.40802875e-01  2.94327457e-03  2.74553485e-02\n",
      " -1.22877255e-01  1.41344756e-01  2.25312248e-01  3.25670004e-01\n",
      "  1.84649993e-02 -9.65782478e-02  1.72021747e-01  3.24707508e-01\n",
      "  5.58669940e-02 -2.07220003e-01 -8.94087479e-02  2.13347495e-01\n",
      " -1.03633493e-01 -2.86620021e-01  4.15201008e-01 -9.16349962e-02\n",
      " -1.98102996e-01  1.21238299e-01 -1.37545750e-01 -1.50137752e-01\n",
      " -6.95754727e-03  1.86879501e-01 -5.73685020e-02 -2.71399990e-02\n",
      " -1.07082896e-01  9.18634981e-02  7.46712461e-02  4.46032509e-02\n",
      " -2.74399757e-01  1.09037496e-01  1.21062741e-01  3.35427523e-01\n",
      " -1.84619501e-01  2.86572501e-02  1.09135240e-01  6.30113930e-02\n",
      "  1.31270498e-01  7.63574988e-02 -1.82839245e-01 -1.30219996e-01\n",
      "  5.23344994e-01 -1.29047632e-01  1.72891244e-01  3.35097462e-02\n",
      " -1.94304749e-01 -1.07482508e-01  2.12738752e-01 -4.07324992e-02\n",
      " -2.42945492e-01 -1.64375007e-02 -1.34829000e-01 -4.67239991e-02\n",
      "  1.97099984e-01  1.10903755e-01  1.48600250e-01  7.04132468e-02\n",
      "  1.43355995e-01 -3.35429996e-01  1.89768016e-01 -3.83202970e-01\n",
      " -1.40029743e-01  5.03121972e-01 -7.79475048e-02  7.83237517e-02\n",
      "  1.84897497e-01  1.54842548e-02  3.38107497e-01 -2.12439999e-01\n",
      " -2.66589999e-01  1.76412970e-01  1.46627501e-01 -7.39411265e-02\n",
      "  4.77957502e-02  1.14010498e-01 -2.49372259e-01 -2.75062501e-01\n",
      " -1.73342526e-02 -4.51627560e-02  1.83018997e-01  1.39599115e-01\n",
      "  7.63474926e-02 -1.70930997e-01 -1.16726495e-01 -2.30366498e-01\n",
      " -6.29459977e-01  1.98060006e-01  3.33459973e-01 -1.91512004e-01\n",
      "  1.81127518e-01 -1.28297508e-01 -4.48192507e-02  7.02579916e-02\n",
      "  1.99540004e-01  5.37825376e-03]\n"
     ]
    }
   ],
   "source": [
    "X_train, tfidf_model, scaler, dictionary, frequency = output_training_features(X[0:400])\n",
    "X_test = output_test_features(X[0], dictionary, tfidf_model, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.frequency = Counter()\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        results = []\n",
    "        for x in X:\n",
    "            result = output_test_features(x, self.dictionary, self.tfidf_model, self.frequency)\n",
    "            results.append(result[0])\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        X_train, tfidf_model, scaler, dictionary, frequency = output_training_features(X)\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.tfidf_model = tfidf_model\n",
    "        self.scaler = scaler\n",
    "        self.dictionary = dictionary\n",
    "        self.frequency = frequency\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "textvectorizer = TextTransformer()\n",
    "clf = GaussianNB()\n",
    "\n",
    "gnb_pipeline = Pipeline([('vectorize', TextTransformer()), \n",
    "                           ('model', clf)])\n",
    "\n",
    "model = transform_pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "Can't Decide       0.00      1.00      0.01        16\n",
      "Not Relevant       0.75      0.51      0.61      6187\n",
      "    Relevant       0.77      0.44      0.56      4673\n",
      "\n",
      "   micro avg       0.48      0.48      0.48     10876\n",
      "   macro avg       0.51      0.65      0.39     10876\n",
      "weighted avg       0.76      0.48      0.59     10876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y, model.predict(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorize', <__main__.TextTransformer object at 0x1a49867eb8>), ('PCA', PCA(copy=True, iterated_power='auto', n_components=20, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)), ('model', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'model__C': (0.001, 1, 1000.0)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(custom_score), verbose=0)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# custom scoring function to improve relevant recall scores\n",
    "def custom_score(y_true, y_pred): \n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    y_true = list(y_true)\n",
    "    y_pred = list(y_pred)\n",
    "    for i, value in enumerate(y_true):\n",
    "\n",
    "        if y_true[i] == y_pred[i] and y_true[i] == 'Relevant':\n",
    "            tp += 1\n",
    "        if y_true[i] == 'Relevant' and y_pred[i] != 'Relevant':\n",
    "            fn += 1\n",
    "    score = tp/(tp + fn + 1e-9)\n",
    "    return score\n",
    " \n",
    "svm = SVC(gamma='scale')\n",
    "svm_pipeline = Pipeline([('vectorize', TextTransformer()), \n",
    "                      ('SVD', TruncatedSVD(n_components=20, random_state=42)),\n",
    "                      ('model', svm)])\n",
    "    \n",
    "scoring = make_scorer(custom_score)\n",
    "\n",
    "params = {'model__C':(1e-3,1,1e3)}\n",
    "\n",
    "clf = GridSearchCV(svm_pipeline, scoring=scoring, param_grid=params, cv=3)\n",
    "\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__C': 1}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([143.98350922, 131.60852599, 149.64854868]),\n",
       " 'std_fit_time': array([18.33198669,  1.24229908,  4.15146226]),\n",
       " 'mean_score_time': array([66.80119189, 63.59841537, 59.44534731]),\n",
       " 'std_score_time': array([4.84149559, 6.39863509, 1.38691445]),\n",
       " 'param_model__C': masked_array(data=[0.001, 1, 1000.0],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'model__C': 0.001}, {'model__C': 1}, {'model__C': 1000.0}],\n",
       " 'split0_test_score': array([0.        , 0.64955071, 0.64890886]),\n",
       " 'split1_test_score': array([0.        , 0.65917843, 0.65275995]),\n",
       " 'split2_test_score': array([0.        , 0.746307  , 0.75080283]),\n",
       " 'mean_test_score': array([0.        , 0.68499989, 0.6841446 ]),\n",
       " 'std_test_score': array([0.        , 0.04351662, 0.04714771]),\n",
       " 'rank_test_score': array([3, 1, 2], dtype=int32),\n",
       " 'split0_train_score': array([0.        , 0.69341894, 0.81605136]),\n",
       " 'split1_train_score': array([0.        , 0.71556982, 0.80995185]),\n",
       " 'split2_train_score': array([0.        , 0.6732991 , 0.80134788]),\n",
       " 'mean_train_score': array([0.        , 0.69409596, 0.80911703]),\n",
       " 'std_train_score': array([0.        , 0.01726359, 0.00603163])}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "Can't Decide       0.00      0.00      0.00        16\n",
      "Not Relevant       0.79      0.88      0.83      6187\n",
      "    Relevant       0.82      0.69      0.75      4673\n",
      "\n",
      "   micro avg       0.80      0.80      0.80     10876\n",
      "   macro avg       0.53      0.52      0.53     10876\n",
      "weighted avg       0.80      0.80      0.79     10876\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y, model.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple train/cross-validation split\n",
    "\n",
    "Score output is the recall for relevant tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "random_state = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, \n",
    "                                                    random_state=random_state, \n",
    "                                                    shuffle=True, \n",
    "                                                    stratify=y)\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X_test, \n",
    "                                              y_test, \n",
    "                                              test_size=0.5, \n",
    "                                              random_state=random_state, \n",
    "                                              shuffle=True, \n",
    "                                              stratify=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorize', <__main__.TextTransformer object at 0x1a49866d30>), ('model', GaussianNB(priors=None, var_smoothing=1e-09))])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48502139800216115"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_score(y_cv, gnb_pipeline.predict(X_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorize', <__main__.TextTransformer object at 0x1a4915b8d0>), ('SVD', TruncatedSVD(algorithm='randomized', n_components=300, n_iter=5,\n",
       "       random_state=42, tol=0.0)), ('model', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_f...obs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=30)\n",
    "rf_pipeline = Pipeline([('vectorize', TextTransformer()), \n",
    "                      ('SVD', TruncatedSVD(n_components=300, random_state=42)),\n",
    "                      ('model', rf)])\n",
    "rf_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.590584878743808"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_score(y_cv, rf_pipeline.predict(X_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6504992867323103"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt_pipeline = Pipeline([('vectorize', TextTransformer()), \n",
    "                      ('SVD', TruncatedSVD(n_components=300, random_state=42)),\n",
    "                      ('model', dt)])\n",
    "dt_pipeline.fit(X_train, y_train)\n",
    "\n",
    "custom_score(y_cv, dt_pipeline.predict(X_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7161198288149556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Can't Decide       0.00      0.00      0.00         2\n",
      "Not Relevant       0.80      0.87      0.83       928\n",
      "    Relevant       0.81      0.72      0.76       701\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      1631\n",
      "   macro avg       0.54      0.53      0.53      1631\n",
      "weighted avg       0.80      0.80      0.80      1631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(learning_rate=.45, n_estimators=400)\n",
    "gbc_pipeline = Pipeline([('vectorize', TextTransformer()), \n",
    "                      ('SVD', TruncatedSVD(n_components=300, random_state=42)),\n",
    "                      ('model', gbc)])\n",
    "gbc_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(custom_score(y_cv, gbc_pipeline.predict(X_cv)))\n",
    "print(metrics.classification_report(y_cv, gbc_pipeline.predict(X_cv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7318116975738491\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Can't Decide       0.00      0.00      0.00         2\n",
      "Not Relevant       0.81      0.88      0.85       928\n",
      "    Relevant       0.82      0.73      0.77       701\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1631\n",
      "   macro avg       0.55      0.54      0.54      1631\n",
      "weighted avg       0.82      0.82      0.81      1631\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr_pipeline = Pipeline([('vectorize', TextTransformer()), \n",
    "                        ('SVD', TruncatedSVD(n_components=300, random_state=42)),\n",
    "                      ('model', lr)])\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(custom_score(y_cv, lr_pipeline.predict(X_cv)))\n",
    "print(metrics.classification_report(y_cv, lr_pipeline.predict(X_cv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2553495007129025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "Can't Decide       0.00      0.00      0.00        13\n",
      "Not Relevant       0.81      0.88      0.84      5259\n",
      "    Relevant       0.82      0.74      0.78      3972\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      9244\n",
      "   macro avg       0.54      0.54      0.54      9244\n",
      "weighted avg       0.81      0.82      0.81      9244\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Can't Decide       0.00      0.00      0.00         3\n",
      "Not Relevant       0.80      0.86      0.83       928\n",
      "    Relevant       0.78      0.71      0.75       701\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      1632\n",
      "   macro avg       0.53      0.52      0.52      1632\n",
      "weighted avg       0.79      0.79      0.79      1632\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Andrew/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr_pipeline = Pipeline([('vectorize', TextTransformer()), \n",
    "                        ('SVD', TruncatedSVD(n_components=300, random_state=42)),\n",
    "                      ('model', lr)])\n",
    "\n",
    "X_traincv = X_train.append(X_cv)\n",
    "y_traincv = y_train.append(y_cv)\n",
    "\n",
    "lr_pipeline.fit(X_traincv, y_traincv)\n",
    "\n",
    "print(metrics.classification_report(y_traincv, lr_pipeline.predict(X_traincv)))\n",
    "print(metrics.classification_report(y_test, lr_pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Not Relevant', 'Not Relevant'], dtype=object)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pipeline.predict([\"Mosque with fluffy pillows to spare\", \n",
    "                     \"Jinwoo has dragged them all to play with fire #WINNER We knew this day was going to come\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Root env",
   "language": "python",
   "name": "root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
